

import numpy as np

import argparse

from datetime import datetime

OBSERVATION_KEY   = "observation"
DESIRED_GOAL_KEY  = "desired_goal"
ACHIEVED_GOAL_KEY = "achieved_goal"


def scale_unit_array(array, scale = 2.0, shift= -1.0):
    """
    Scales an array in range [0, 1] by the given factor then shifting it by a constant.
    By default, this scale the unit array to [-1, 1].

        Parameters
            array (np.ndarray): A numpy array with minimum zero and maximum one
            scale (float): Scaling factor to multiply the unit array by. Default is 2.0
            shift (float): Shift factor to add to the array. Default is -1.0

        Returns
            transformed_array (np.ndarray): An array generated by array * scale + shift
    """
    return (array * scale) + shift 


def get_gym_environment(args):
    """
    Creates an gym environment based on the user specified RLBench task. We fix a few option.
    Mainly, the observation mode for all tasks is "visiondepth" to get the RGB and depth
    information from the environment.

        Parameters:
            args (parsed_args): Arguments for this script

        Returns
            env (Env): An RLBench environment that conforms to the gym API
    """
    import gym

    # NOTE: This registers all the gym environments.
    import rlbench.gym

    from rlbench import VisualRandomizationConfig

    '''
    Because of their custom edits, we can only use the following RLBench Task classes:
    - EmptyContainer
    - PickAndLift
    - ReachTargetEasy
    - SlideBlockToTarget 

    The gym.make command will essentially call RLBenchEnv which takes the following
    arguments with the specified defaults

        task_class,                     # Specified by gym. Creates the task environment.
        observation_mode='state',       # Must be "state" or "vision" or "visiondepth" or "visiondepthmask"
        randomization_mode="none",      # Must be provided. If "random" will run domain randomization.
        rand_config=None,               # A VisualRandomizationConfig. If running domain randomization, must provide this.
        img_size=256,                   # Number of pixels in the NxN images returned by the environment. The use either 256 or 64.           
        special_start=[],               # A list of actions to take at the beginning.
        fixed_grip=-1,                  # Some hack for the gripper. Best to keep as -1.
        headless=True,                  # If true will not display the agent in CoppeliaSim viewer
        force_randomly_place=False,     # has to do with object placement when re-setting a scene
        force_change_position=False,    # Has to do with robot joint position when re-setting a scene
        sparse=False,           # Unclear. This was a MLR-custom addition. HAs to do with rewards.
        not_special_p = 0,      # Used for initial position in episodes. 0 is off.
        ground_p = 0,           # Used for initial position in episodes. 0 is off.
        special_is_grip=False,  # Used in initializing an episode. Use is unclear.
        altview=False,          # If true will take images from the non-front camera.
        procedural_ind=0,       # Used in the EmptyContainer task to indicate which procedurally generated object is removed
        procedural_mode='same', # Indicates what mode to operate the EmptyContainer task in 'random' or 'increase'
        procedural_set = [],    # Set of procedural objects for the EmptyContainer Task
        is_mesh_obs=False,      # Has to do with procedural objects in the EmptyContainer task
        blank=False,            # If true, all observations are zeros
        COLOR_TUPLE=[1,0,0],    # What color to set objects in the EmptyContainer task to be
    '''
    

    rand_config = VisualRandomizationConfig(
            # NOTE: Using the RLBench default textures for Domain Randomization
            # image_directory='/anonymized/experiment_textures/train/top10',
            image_directory='/home/andrew/Dev/RPVMLR/RLBench/tests/unit/assets/textures',
            whitelist = ['Floor', 'Roof', 'Wall1', 'Wall2', 'Wall3', 'Wall4', 'diningTable_visible'],
            apply_floor=True
    )
    env_name = 'reach_target_easy-visiondepth'
    env_name += '-random' if args.domain_randomization else ''
    env_name += '-v0'

    env = gym.make(
        # Fixed task on reach target
        env_name,
        
        # If display is True then set headless to False
        headless = not( args.display ),
        img_size=args.image_size,
        
        # Fixed values here for things I don't really understand
        sparse = True,
        force_randomly_place  = True,
        force_change_position = False,

        rand_config=rand_config
    )
    return env


def wrap_environment(env, args):
    """
    Wraps the environment to modify the observations.

        Parameters:
            env (RLBenchEnv): The gym/RLBench environment for the task.
        
        Returns:
            wrapped_env (TransformObservationWrapper): A wrapped environment which modifies observations. 
    """
    from rlkit.envs.wrappers import TransformObservationWrapper

    # NOTE: Figure this part out with the main_t_tf
    def isolate_depth_observation(obs):
        # <class 'numpy.ndarray'> (1, 64, 64) -> (4096)
        obs["observation"] = obs["observation"][1].flatten()
        # print("[DEBUG] isolate_depth_observation", type(obs["observation"]), obs["observation"].shape)
        return obs

    def isolate_rgb_observation(obs):
        # <class 'numpy.ndarray'> (3, 64, 64) -> (1228)
        obs["observation"] = obs["observation"][0].flatten()
        # print("[DEBUG] isolate_rgb_observation", type(obs["observation"]), obs["observation"].shape)
        return obs

    def combine_rgb_and_depth(obs):
        # TODO: It would be neat to implement this
        raise NotImplementedError

    if args.depth:
        transformation_function = isolate_depth_observation
    else:
        transformation_function = isolate_rgb_observation

    def wrapper_unit_array_scale(obs):
        obs['observation'] = scale_unit_array(obs['observation'])
    return TransformObservationWrapper(env, transformation_function)


def get_td3_trainer_and_policy(args, env, her = False):
    """
    Gets the RLKit's Twin Delayed Deep Deterministic Policy Gradient (TD3) trainer based 
    on the user's arguments.

    Please note that it may take a while (tens of seconds to a minute) for this part of the script to run.

        Parameters:
            args (parsed_args): Arguments for this script
            env (RLBenchEnv): The gym/RLBench environment for the task. Used for network input/output shapes.
            her (bool): HER changes the structure of the Action-Value network. If True, this will include
                        additional inputs to this network.

        Returns
            td3_trainer (TD3Trainer): An RLKit TD3Trainer
            exploration_policy (PolicyWrappedWithExplorationStrategy): The policy network (same policy as given to
                                                                       td3_trainer) paired with a gaussian-epsilon
                                                                       exploration strategy.
    """
    from torch import nn

    from rlkit.torch.td3.td3 import TD3Trainer
    from rlkit.torch.conv_networks import FlattenCNN, TanhCNNPolicy

    from rlkit.exploration_strategies.base import PolicyWrappedWithExplorationStrategy
    from rlkit.exploration_strategies.gaussian_and_epsilon_strategy import GaussianAndEpsilonStrategy


    trainer_kwargs = dict(
        policy_learning_rate = args.learning_rate,
        qf_learning_rate     = args.learning_rate,
        target_policy_noise  = 0.2,
        discount  = 0.95,

        # The following are left as their defaults
        policy_and_target_update_period = 2,
        target_policy_noise_clip        = 0.5,
        reward_scale = 1.0,
        qf_criterion = None,
        tau = 0.005,
    )

    actor_kwargs = dict(    # variant['policy_kwargs']
        # NOTE: Values from starter/state.py
        # hidden_sizes=[256, 256, 256]
    )

    critic_kwargs = dict(   # variant['qf_kwargs']
        # NOTE: Values from starter/state.py
        # hidden_sizes=[256, 256, 256]
    )

    # Just for depth
    image_channels = 1
    if not args.depth:
        # Or 3 for RGB
        image_channels += 2

    # Both actor and critic get the same CNN structure: same input but different outputs
    cnn_kwargs = dict(      # variant['conv_args']
        # Gives the actors and critic the same hidden sizes and network structure
        hidden_sizes   = [1024, 512, 256],
        input_width    = args.image_size,
        input_height   = args.image_size,
        input_channels = image_channels,  # [R,G,B or [Depth]
        kernel_sizes   = [4,4,3],
        n_channels     = [32,64,64],
        strides        = [2,1,1],
        paddings       = [0,0,0],
        batch_norm_conv   = False,
        batch_norm_fc     = False,
        hidden_init       = nn.init.orthogonal_,
        hidden_activation = nn.ReLU(),
        init_w = 1e-4,
    )

    # Defined in RLBenchEnv: may take the shape from either the high
    # or low bounds of the action space. If this is not given, we guess 4.
    action_dim = env.action_space.low.size if env is not None else 4
    goal_dim = env.observation_space.spaces['desired_goal'].low.size if env is not None else 4

    # TODO: Move get actor and critic methods to a separate function. Maybe even separate module
    def get_actor_network():
        '''
        Returns a PyTorch/RLKit neural network structure for estimating the action-value
        function, otherwise known as the critic.

        These networks take the observation image as an input.

            Returns:
                critic_network: Neural network structure
        '''
        return TanhCNNPolicy(
            output_size=action_dim,
            **actor_kwargs,
            **cnn_kwargs
        )

    def get_critic_network():
        '''
        Returns a PyTorch/RLKit neural network structure for estimating the action-value
        function, otherwise known as the critic.

        These networks take the observation image in as well as the action.

            Returns:
                critic_network: Neural network structure
        '''
        return FlattenCNN(
            added_fc_input_size=action_dim + goal_dim,
            output_size=1,
            **critic_kwargs,
            **cnn_kwargs
        )
    
    # Epsilon-random strategy that adds gaussian noise
    # to the non-random actions
    strategy = GaussianAndEpsilonStrategy(
        action_space = env.action_space, 
        epsilon   = 0.3,
        max_sigma = 0.01,
        min_sigma = 0.01, #constant sigma 0
        decay_period=1000000
    )

    exploration_policy = PolicyWrappedWithExplorationStrategy(
        exploration_strategy=strategy,
        policy=policy,
    )

    td3_trainer = TD3Trainer(
        policy = policy,
        target_policy = get_actor_network(),
        qf1 = get_critic_network(),
        target_qf1 = get_critic_network(),
        qf2 = get_critic_network(),
        target_qf2 = get_critic_network(),
        **trainer_kwargs
    )

    return td3_trainer, exploration_policy


def get_t3d_with_her_trainer(args, env):
    """
    Gets the RLKit's Twin Delayed Deep Deterministic Policy Gradient (TD3) trainer based 
    on the user's arguments and wraps it in a Hindsight Experience Replay class.

        Parameters:
            args (parsed_args): Arguments for this script
            env (RLBenchEnv): the gym/RLBench environment for the task. Used only for network input/output shapes.

        Returns
            her_trainer (HERTrainer): A TD3Trainer wrapped in HER
            exploration_policy (PolicyWrappedWithExplorationStrategy): The policy network (same policy as given to
                                                                       td3_trainer) paired with a gaussian-epsilon
                                                                       exploration strategy.
    """
    from rlkit.torch.her.her import HERTrainer

    td3_trainer, exploration_policy = get_td3_trainer_and_policy(args, env, her=True)
    return HERTrainer(td3_trainer), exploration_policy


def get_replay_buffer(args, env, multiprocessing = False):
    """
    Creates an image relabeling buffer based on the user inputs

        Parameters:
            args (parsed_args): Arguments for this script
            env  (RLBenchEnv):  The gym/RLBench environment for the task. Used only for network input/output shapes.

        Returns
            her_trainer (imgObsDictRelabelingBuffer): A relabeling buffer from RLKit             
    """
    if multiprocessing == False:
        '''
        Per the `starter/state.py` example.

        Fuck. This doesn't work at all. Wait. Why did I say that? Ah. It was because of issues with
        the goal keys not existing for the desired and achieved goals. But I added them - copied what
        was shown on the 'state' observation mode from RLBenchEnv to the rest of them. 

        '''
        from rlkit.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer

        replay_buffer_kwargs = dict(
            max_size = args.replay_buffer_size,
            fraction_goals_rollout_goals = 0.2,  # equal to k = 4 in HER paper
            fraction_goals_env_goals = 0,
        )

        replay_buffer = ObsDictRelabelingBuffer(
            env=env,
            observation_key   = OBSERVATION_KEY,
            desired_goal_key  = DESIRED_GOAL_KEY,
            achieved_goal_key = ACHIEVED_GOAL_KEY,
            **replay_buffer_kwargs 
        )
    else:
        '''
        Per the `starter/mid_level_train.py` example

        It is super unclear here how the multiprocessing environment is supposed to work.
        I have included what I have figured out but I'm going to stop here for now.
        '''
        from rlkit.data_management.obs_dict_replay_buffer import imgObsDictRelabelingBuffer
        from rlkit.envs.vec_envs import SubprocVecEnv

        import time

        # NOTE: Different kwargs
        replay_buffer_kwargs = dict(
            max_size = args.replay_buffer_size,  # Default is 1e6
            k = 4,
        )

        def isolate_depth_observation(obs):
            return obs["observation"][1].flatten()

        def isolate_rgb_observation(obs):
            return obs["observation"][0].flatten()

        def combine_rgb_and_depth(obs):
            # TODO: It would be neat to implement this
            raise NotImplementedError

        if args.depth:
            transformation_function = isolate_depth_observation
        else:
            transformation_function = isolate_rgb_observation
        
        # NOTE: This starts new RLBench environments as sub-processes. I honestly don't know
        #       how it works. But if there are error this is a good place to investigate.
        env_function = lambda : get_gym_environment(args)
        rerendering_env = SubprocVecEnv([env_function for _ in range(args.num_cpu)])
        
        # They included this 1 second sleep timer. Presumably to let each CoppeliaSim 
        # environment finish loading properly. Feels like a hack. But we'll keep it.
        time.sleep(1)

        replay_buffer = imgObsDictRelabelingBuffer(
            env=env,
            # NOTE: Somehow, this relates to  multi-processing??
            rerendering_env   = rerendering_env,
            observation_key   = OBSERVATION_KEY,
            desired_goal_key  = DESIRED_GOAL_KEY,
            achieved_goal_key = ACHIEVED_GOAL_KEY,
            # t_fn transforms the observations from RLBench
            t_fn = transformation_function,
            **replay_buffer_kwargs 
        )
    return replay_buffer


def get_path_collector(env, exploration_policy, evaluation = False):
    """
    Creates a path collector for either exploration or evaluation of a policy.

        Parameters:
            env (RLBenchEnv): The gym/RLBench environment for the task.
            exploration_policy (PolicyWrappedWithExplorationStrategy): The policy network wrapped with its
                                                                       exploration strategy.
            evaluation (bool): Default False. If True, the path collector policy will be the policy without.
                               the exploration (i.e., just the network). If false it will be the policy
                               with its exploration strategy.
    """
    from rlkit.samplers.data_collector import GoalConditionedPathCollector

    # Isolate the policy from the exploration strategy if the evaluation flag is set
    policy = exploration_policy.policy if evaluation else exploration_policy

    path_collector = GoalConditionedPathCollector(
        env    = env,
        policy = policy,
        observation_key  = OBSERVATION_KEY,
        desired_goal_key = DESIRED_GOAL_KEY,
    )
    return path_collector


def run_algorithm(args, env, trainer, exploration_policy):
    """
    Runs the RLKit and PyTorch algorithm
    """
    from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
    import rlkit.torch.pytorch_util as ptu

    # TODO: Move some of these to argparser
    algorithm_kwargs = dict(          # variant['algorithm_kwargs']
        num_epochs = int(1e5),        # 3e6 train steps  # FIXME Increase the number of epochs here!
        num_eval_steps_per_epoch = 0, # 2 rollouts per epoch
        num_epochs_per_eval = 0,

        num_trains_per_train_loop = 50,
        num_expl_steps_per_train_loop = 100,
        
        random_before_training = True,
        min_num_steps_before_training = args.steps_before_training,

        max_path_length = args.max_path_length,
        batch_size = 100,
    )

    algorithm = TorchBatchRLAlgorithm(
        trainer = trainer,
        exploration_env = env,
        evaluation_env  = None,
        exploration_data_collector = get_path_collector(env, exploration_policy),
        evaluation_data_collector  = None,
        replay_buffer = get_replay_buffer(args, env, multiprocessing=False),
        **algorithm_kwargs
    )

    print("Running algorithm!")

    algorithm.to(ptu.device)
    algorithm.train()


def check_save_path(args):
    """
    Verifies that the name given will create a valid directory to save the networks in and
    will no overwrite any other files.

        Parameters:
            args (parsed_args): Arguments for this script

        Returns:
            path (Path): Path to the directory all the files are saved in

        Raises:
            SystemExit if a folder of the name already exists and the user does not
            want to overwrite them.
    """
    from pathlib import Path
    import os
    
    root_path = str( Path(__file__).resolve().parent )
    path = os.path.join(root_path, "networks", args.name)

    if os.path.isdir(path):
        response = None
        print("Files may already exist in %s" % path)
        while not response in ["yes", "no"]:
            response = input("Overwrite these files? [yes|no]: ").lower()
    
        if response == "yes":
            pass
        else:
            raise SystemExit("Exiting to not overwrite existing files.")
    
    else:
        os.makedirs(path)
    
    return path


def save_td3_networks(trainer, path):
    """
    Saves each network from the trainer (even though just the policy is needed).
    Networks are saved in /ISE_7202/networks/[NAME] where [NAME] is an input argument.

    Note that this will overwrite any existing files. The `check_save_path` attempts to verify
    for users if this is intentional. Best practice is to call that first.

        Parameters:
            trainer (HERTrainer or TD3Trainer): Network to be saved.
            path (Path): Path to the directory all the files are saved in
    """
    from torch import save
    from torch.nn import Module

    import os

    network_dict = trainer.get_snapshot()

    if not os.path.isdir(path):
        os.makedirs(path)

    for network_name, network in network_dict.items():
        if not isinstance(network, Module):
            continue
        fname = network_name + ".pt"        
        fpath = os.path.join(path, fname)
        save(network, fpath)


def set_pt_device(args):
    """
    Sets PyTorch to use either the CPU or GPU based on the user's input.
    This device is used for the rest of the script.

        Parameters:
            args (parsed_args): Arguments for this script
    """
    import rlkit.torch.pytorch_util as ptu
    if not( args.use_cpu ):
        ptu.set_gpu_mode(True)


def main(args):
    """
    Entry point for the script. Accepts the parsed arguments and uses them to run
    the rest of the functions.

        Parameters:
            args (parsed_args): Arguments for this script
    """
    save_path = check_save_path(args)
    try:
        explore_env = get_gym_environment(args)
        explore_env = wrap_environment(explore_env, args)

        # NOTE: Had issues getting an evaluation environment create: seemed to freeze.
        #       Not going to trouble shoot now. Probably needs its own thread.
        # evaluation_env = get_gym_environment(args)
        # evaluation_env = wrap_environment(evaluation_env, args)

        set_pt_device(args)

        td3_her_trainer, exploration_policy = get_t3d_with_her_trainer(args, explore_env)

        run_algorithm(args, explore_env, td3_her_trainer, exploration_policy)

        save_td3_networks(td3_her_trainer, save_path)
        
        print("[INFO] Finished main!")
    except Exception as e:
        raise e
    finally:
        print("[INFO] Closed environment(s)")
        # Always close the environment!
        explore_env.close()
        # evaluation_env.close()


if __name__ == "__main__":
    print('[INFO] Running main script')

    parser = argparse.ArgumentParser(description="Testing script for ISE 7202 final project")

    # ----------------------  The core arguments  ---------------------- #
    parser.add_argument(
        "--name", "-n",
        type = str,
        default = datetime.now().strftime('%Y-%m-%d-%H-%M-%S'),
        help = "Name for this network. Used in saving the weights. Default is the timestamp when training started."
    )

    parser.add_argument(
        "--image-size", "-sz",
        type = int,
        default = 64,
        help = "Number of pixels in the square RGB and Depth image. Recommended values are 64 or 256."
    )

    parser.add_argument(
        '--max-path-length',
        type = int,
        default  = 50,
        help="Number of steps per rollout."  # NOTE: What is a rollout?
    )

    parser.add_argument(
        '--steps-before-training',
        type = int,
        default = 1000,
        help = "The (minimum) number of steps before starting to train."
    )

    parser.add_argument(
        '--replay-buffer-size', '-rb',
        type = int,
        default = int(1e6),
        help = "Size of replay buffer."
    )

    parser.add_argument(
        "--depth",
        action  = 'store_true',
        default = False,
        help = "Include this flag to train the agent on depth instead of RGB."
    )

    parser.add_argument(
        '--domain-randomization','-dr',
        action='store_true',
        default=False,
        help="Include this flag to use the chosen environment with domain randomization"
    )


    # TODO: Avoid multiprocessing?
    parser.add_argument(
        '--num-cpu',
        default = 3,
        help = "Number of CPUS to use for the buffer. Each CPU gets one environment."
    )

    # ----------------------  Additional arguments  ---------------------- #
    parser.add_argument(
        "--display", "-d",
        action  = 'store_true',
        default = False,
        help = "Include this flag to display the agent in the CoppeliaSim environment."
    )

    parser.add_argument(
        "--use-cpu",
        action  = 'store_true',
        default = False,
        help = "Include this flag to train the algorithm on CPU only. Default is to use GPU."
    )

    parser.add_argument(
        "--learning-rate", "-lr",
        type = float,
        default = 1e-4,
        help = "Learning rate for both the policy (actor) networks and the critic (Q/value function) networks."
    )


    args = parser.parse_args()
    
    # The RLBench `visiondepth` observation mode returns a list for the "observation" key of
    # [RGB array, Depth Array]. We need to select only one of these.
    VIEW_SELECTOR = lambda obs : obs[1] if args.depth else lambda obs : obs[0]

    main(args)
