
## For `dataset_generator.py`


Description: Deterministic dataset generator for State Representation Learning (SRL) training (can be used for environment testing)

Recomended argument
```bash
python -m environments.dataset_generator \
--num-cpu=3 \
--num-episode=50 \
--max_steps_per_epoch 150 \
--name=reach \
--env=reach \
--force \
```

`--num-cpu`,
- type=int, 
- default=1,
- help='number of cpu to run on'

`--num-episode`,
- type=int,
- default=50,
- help='number of episode to run'

`--max_steps_per_epoch`,
- type=int,
- default=200,
- help='max num steps per epoch'

CUSTOM ARGS. want to update eventually, i.e., specify a specific path for dr

`--dr`,
- action='store_true`,
- default=False,
- help="Include this flag to use the chosen environment with domain randomization"

`--alt`,
- action='store_true`,
- default=False,
- help="Include this flag to use the chosen environment with alternate view"

`--special_start`,
- action='store_true`,
- default=False,
- help="Include this flag to use the chosen environment with the special start"

`--save-path`,
- type=str,
- default='robotics-rl-srl/data/`,
- help='Folder where the environments will save the output'

`--name`,
- type=str,
- default='UNSETNAME`,
- help='Folder name for the output'

`--env`,
- type=str,
- default='push_rotate`, help='The environment wanted`,
- choices= `reach`, `push_rotate`, `push_free`, `pick_and_lift`, `pick_and_lift_nograb`, `procedural`

`--display`,
- action='store_true`,
- default=False

`--no-record-data`,
- action='store_true`,
- default=False

`--seed`,
- type=int,
- default=0,
- help='the seed'

`-f`, `--force`,
- action='store_true`,
- default=False,
- help='Force the save, even if it overrides something else, including partial parts if they exist'

From their notes" "TODO Change this argument to be for the diff types of tasks"

`--multi-view`, 
- action='store_true`,
- default=False,
- help='Set a second camera to the scene'

`--reward-dist`,
- action='store_true`,
- default=False,
- help='Prints out the reward distribution when the dataset generation is finished'

`--run-ppo2`,
- action='store_true`,
- default=False,
- help='runs a ppo2 agent instead of a random agent'

`--ppo2-timesteps`,
- type=int,
- default=1000,
- help='number of timesteps to run PPO2 on before generating the dataset'


## For `train.py`

Description: Train mid-level features from a dataset.


```bash
python train.py --name insert_name \
                --feature_task normal \
                --batch_size 32 \
                --num_epochs 500 \
                --data_path /path/to/train/data/
                --lr 3e-4
```

`--name`,
- type=str,
- required=True,
- help='Folder name for the output. Dont end with trailing slash'

`--feature_task`,
- type=str,
- required=True,
- help='Feature task to train. One of sobel, normal, segment_semantic, or depth'

`--batch_size`,
- type=int,
- default=32,
- help='Batch size to train with'

`--num_epochs`,
- type=int,
- default=200,
- help='Number of epochs to train with'

`--lr`,
- type=float,
- default=3e-4,
- help='Learning rate'

`--data_path`,
- type=str,
- default='example_path',
- help='Path to folders generated by robotics-rl-srl'

`--model_path`,
- type=str,
- default="",
- help='Path to model to initialize for finetuning. Default is nothing.'

`--checkpoint_interval`,
- type=int,
- default=20,
- help=''

## For `mid_level_train.py`

To train a policy:

To train without any mid level features
```bash
python mid_level_train.py   --feature_task depth \
                            --env reach \
                            --mlf 0 \
                            --models_path /path/to/representations
                            --noise eps 
                            --max_path_length 50
```

To train with (__TODO__)

```bash
python mid_level_train.py   --feature_task normal \
                            --env reach \
                            --models_path /path/to/representations
                            --noise eps
                            --lr 1e-4
                            --max_path_length 50
                            --min_num_steps_before_training 1000
                            --special_name ExpName
```

`--feature_task`,
- type=str,
- required=True,
- help='Feature task to test. One of sobel, normal, segment_semantic, or depth'

`--models_path`,
- type=str,
- required=True,
- help='path to root of MLR model. Expects path/normal to contain normal logs. UNUSED FOR NON MLR'

`--env`,
- type=str,
- required=True,
- help='Env to run on. one of the keys in envs.'

`--lr`,
- type=float,
- default=1e-4,
- help='policy/q learning rate'

`--dr`,
- type=bool
- action='store_true`,
- default=False,
- help="Include this flag to use the chosen environment with domain randomization"

`--noise`,
- type=str,
- default="eps",
- help="eps for 0.3 eps greedy, gaussian for gaussian noise"

`--max_path_length`,
- type=int,
- required=True,
- help="#steps / rollout "

`--min_num_steps_before_training`,
- type=int,
- default=1000, 
- help="#steps before starting to train "

`--special_name`,
- type=str,
- default="",
- help="extra descrpition to run. usually for reruns "

`--mlf`,
- type=int,
- default=1,
- help="Whether or not to use mid level features."

`--replay_buffer_size`,
- type=int,
- default=int(1e6),
- help="Size of replay buffer."

`--trial`,
- type=int,
- default=0,
- help="random seed trial"

`--not_special_p`,
- type=float,
- default=0,
- help="percent episodes to start from above"
`--ground_p`,
- type=float,
- default=0.5,
- help="percent episodes to have ground goals"

`--special_is_grip`,
- type=int,
- default=0,
- help="whether to grip or hover. 0 for hover, 1 for grip."

`--alt`,
- type=bool,
- default=False,
- help="True to replace with altview. Both for both. False for frontcam. "

`--num_cpus`,
- type=int,
- default=16,
- help="Number of envs to use."

`--checkpoint`,
- type=int,
- default=-1,
- help="to select a checkpointed model. -999 for random, -9 for init taskonomy, -1 for best, +int % 20 for checkpoints."

`--blank`,
- type=bool,
- action="store_true",
- default=False,
- help="Whether to use envs with pixel observations as all 0s. "

`--readout`,
- type=bool,
- action="store_true",
- default=False,
- help="Whether to use readouts directly to train.
